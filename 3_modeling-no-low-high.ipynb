{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "import mord\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('features-labels.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, 3:-1]  # Exclude the first three columns and the last column\n",
    "y = data['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Independet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "We have a total of 40 subjects. To ensure that our model is trained, validated, and tested, we are splitting these subjects into 32 - 4 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects in the test set: [20 17 16 27  5 13]\n",
      "Dataset size: (15360, 20)\n",
      "Train set size: (13056, 20)\n",
      "Test set size: (2304, 20)\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 6 subjects for the test set\n",
    "np.random.seed(42)  # For reproducibility\n",
    "test_subjects = np.random.choice(range(1, 41), 6, replace=False)\n",
    "print('Subjects in the test set:', test_subjects)\n",
    "\n",
    "# Extract test set\n",
    "test_set = data[data['file'].str.contains('|'.join([f'sub_{i}' for i in test_subjects]))]\n",
    "\n",
    "# Extract train set\n",
    "train_set = data[~data['file'].str.contains('|'.join([f'sub_{i}' for i in test_subjects]))]\n",
    "\n",
    "# Display the sizes of the datasets\n",
    "print(f\"Dataset size: {data.shape}\")\n",
    "print(f\"Train set size: {train_set.shape}\")\n",
    "print(f\"Test set size: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels for train, validation, and test sets\n",
    "X_train = train_set.iloc[:, 3:-1]\n",
    "y_train = train_set['Labels']\n",
    "\n",
    "X_test = test_set.iloc[:, 3:-1]\n",
    "y_test = test_set['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal classification via Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4427083333333333\n",
      "Test F1 Score: 0.32375290364439235\n",
      "Test Cohen's Kappa: 0.04129420304884379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marioroca/stess-detection-sam40/sam40/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model on the combined dataset\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the combined validation and test sets\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa\n",
    "combined_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "combined_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "combined_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {combined_accuracy}\")\n",
    "print(f\"Test F1 Score: {combined_f1}\")\n",
    "print(f\"Test Cohen's Kappa: {combined_kappa}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4309895833333333\n",
      "Test F1 Score: 0.2667322568704218\n",
      "Test Cohen's Kappa: 0.004597798118699203\n"
     ]
    }
   ],
   "source": [
    "# Train the ordinal logistic regression model on the training dataset\n",
    "model = mord.LogisticAT(alpha=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa\n",
    "combined_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "combined_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "combined_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {combined_accuracy}\")\n",
    "print(f\"Test F1 Score: {combined_f1}\")\n",
    "print(f\"Test Cohen's Kappa: {combined_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Test Accuracy: 0.4683159722222222\n",
      "Gradient Boosting Test F1 Score: 0.38965098538934956\n",
      "Gradient Boosting Test Cohen's Kappa: 0.1048411651273723\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boosting model on the training dataset\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa for the Gradient Boosting model\n",
    "gb_accuracy = accuracy_score(y_test, y_test_pred_gb)\n",
    "gb_f1 = f1_score(y_test, y_test_pred_gb, average='weighted')\n",
    "gb_kappa = cohen_kappa_score(y_test, y_test_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting Test Accuracy: {gb_accuracy}\")\n",
    "print(f\"Gradient Boosting Test F1 Score: {gb_f1}\")\n",
    "print(f\"Gradient Boosting Test Cohen's Kappa: {gb_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 0.4505208333333333\n",
      "XGBoost Test F1 Score: 0.4230452619612512\n",
      "XGBoost Test Cohen's Kappa: 0.1187423864493301\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, random_state=42)\n",
    "\n",
    "# Train the XGBoost model on the training dataset\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa for the XGBoost model\n",
    "xgb_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "xgb_kappa = cohen_kappa_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Test Accuracy: {xgb_accuracy}\")\n",
    "print(f\"XGBoost Test F1 Score: {xgb_f1}\")\n",
    "print(f\"XGBoost Test Cohen's Kappa: {xgb_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participand Dependent Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we trained the model using data from 34 participants and evaluated its predictive performance on the remaining 6 participants. Now, let's build a participant-dependent model, where we train a separate model for each participant using data from their first two trials and test it on the remaining trial for each activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n",
      "Mean Accuracy: 0.5119140625\n",
      "Mean F1 Score: 0.45545114482850657\n",
      "Mean Cohen's Kappa: 0.21450791951470638\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each participant\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "kappas = []\n",
    "\n",
    "# Loop through each participant\n",
    "for subject in range(1, 41):\n",
    "    print(f\"{subject}\", end=' ')\n",
    "    # Extract data for the current subject\n",
    "    subject_data = data[data['file'].str.contains(f'sub_{subject}_')]\n",
    "    \n",
    "    # Split data into training and testing sets based on trial number\n",
    "    train_data = subject_data[subject_data['file'].str.contains('trial1|trial2')]\n",
    "    test_data = subject_data[subject_data['file'].str.contains('trial3')]\n",
    "    \n",
    "    # Define features and labels for training and testing sets\n",
    "    X_train = train_data.iloc[:, 3:-1]\n",
    "    y_train = train_data['Labels']\n",
    "    X_test = test_data.iloc[:, 3:-1]\n",
    "    y_test = test_data['Labels']\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy, F1 score, and Cohen's kappa\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    kappas.append(kappa)\n",
    "\n",
    "print() # Print a newline\n",
    "\n",
    "# Aggregate metrics\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_kappa = np.mean(kappas)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Cohen's Kappa: {mean_kappa}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n",
      "Mean Accuracy (XGBoost): 0.4884765625\n",
      "Mean F1 Score (XGBoost): 0.43858272651353447\n",
      "Mean Cohen's Kappa (XGBoost): 0.14603988632802797\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each participant\n",
    "accuracies_xgb = []\n",
    "f1_scores_xgb = []\n",
    "kappas_xgb = []\n",
    "\n",
    "# Loop through each participant\n",
    "for subject in range(1, 41):\n",
    "    print(f\"{subject}\", end=' ')\n",
    "    # Extract data for the current subject\n",
    "    subject_data = data[data['file'].str.contains(f'sub_{subject}_')]\n",
    "    \n",
    "    # Split data into training and testing sets based on trial number\n",
    "    train_data = subject_data[subject_data['file'].str.contains('trial1|trial2')]\n",
    "    test_data = subject_data[subject_data['file'].str.contains('trial3')]\n",
    "    \n",
    "    # Define features and labels for training and testing sets\n",
    "    X_train = train_data.iloc[:, 3:-1]\n",
    "    y_train = train_data['Labels']\n",
    "    X_test = test_data.iloc[:, 3:-1]\n",
    "    y_test = test_data['Labels']\n",
    "\n",
    "    # Ensure all classes are present in y_train. This is necessary for XGBoost to work.\n",
    "    for missing_class in [0, 1, 2]:\n",
    "        if missing_class not in y_train.unique():\n",
    "            dummy_features = X_train.iloc[0, :].values  # Use a sample feature row as dummy\n",
    "            X_train = pd.concat([X_train, pd.DataFrame([dummy_features], columns=X_train.columns)])\n",
    "            y_train = pd.concat([y_train, pd.Series([missing_class])])\n",
    "    \n",
    "    # Define the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, random_state=42)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    if y_test_pred_xgb.ndim == 2:\n",
    "        y_test_pred_xgb = np.argmax(y_test_pred_xgb, axis=1)  # Convert probabilities to class labels\n",
    "    \n",
    "    # Calculate accuracy, F1 score, and Cohen's kappa\n",
    "    accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "    f1_xgb = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "    kappa_xgb = cohen_kappa_score(y_test, y_test_pred_xgb)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies_xgb.append(accuracy_xgb)\n",
    "    f1_scores_xgb.append(f1_xgb)\n",
    "    kappas_xgb.append(kappa_xgb)\n",
    "\n",
    "print()  # Print a newline\n",
    "# Aggregate metrics\n",
    "mean_accuracy_xgb = np.mean(accuracies_xgb)\n",
    "mean_f1_xgb = np.mean(f1_scores_xgb)\n",
    "mean_kappa_xgb = np.mean(kappas_xgb)\n",
    "\n",
    "print(f\"Mean Accuracy (XGBoost): {mean_accuracy_xgb}\")\n",
    "print(f\"Mean F1 Score (XGBoost): {mean_f1_xgb}\")\n",
    "print(f\"Mean Cohen's Kappa (XGBoost): {mean_kappa_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam40",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
