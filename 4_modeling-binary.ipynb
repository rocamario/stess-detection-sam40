{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('binary-features-labels.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, 3:-1]  # Exclude the first three columns and the last column\n",
    "y = data['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Independet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "We have a total of 40 subjects. To ensure that our model is trained, validated, and tested, we are splitting these subjects into 32 - 4 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects in the test set: [20 17 16 27  5 13]\n",
      "Dataset size: (15360, 20)\n",
      "Train set size: (13056, 20)\n",
      "Test set size: (2304, 20)\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 6 subjects for the test set\n",
    "np.random.seed(42)  # For reproducibility\n",
    "test_subjects = np.random.choice(range(1, 41), 6, replace=False)\n",
    "print('Subjects in the test set:', test_subjects)\n",
    "\n",
    "# Extract test set\n",
    "test_set = data[data['file'].str.contains('|'.join([f'sub_{i}' for i in test_subjects]))]\n",
    "\n",
    "# Extract train set\n",
    "train_set = data[~data['file'].str.contains('|'.join([f'sub_{i}' for i in test_subjects]))]\n",
    "\n",
    "# Display the sizes of the datasets\n",
    "print(f\"Dataset size: {data.shape}\")\n",
    "print(f\"Train set size: {train_set.shape}\")\n",
    "print(f\"Test set size: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels for train, validation, and test sets\n",
    "X_train = train_set.iloc[:, 3:-1]\n",
    "y_train = train_set['Labels']\n",
    "\n",
    "X_test = test_set.iloc[:, 3:-1]\n",
    "y_test = test_set['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal classification via Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7456597222222222\n",
      "Test F1 Score: 0.6542085401800191\n",
      "Test Cohen's Kappa: 0.02170283806343909\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model on the combined dataset\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the combined validation and test sets\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa\n",
    "combined_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "combined_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "combined_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {combined_accuracy}\")\n",
    "print(f\"Test F1 Score: {combined_f1}\")\n",
    "print(f\"Test Cohen's Kappa: {combined_kappa}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Test Accuracy: 0.7621527777777778\n",
      "Gradient Boosting Test F1 Score: 0.6792214321626087\n",
      "Gradient Boosting Test Cohen's Kappa: 0.09121061359867333\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boosting model on the training dataset\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa for the Gradient Boosting model\n",
    "gb_accuracy = accuracy_score(y_test, y_test_pred_gb)\n",
    "gb_f1 = f1_score(y_test, y_test_pred_gb, average='weighted')\n",
    "gb_kappa = cohen_kappa_score(y_test, y_test_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting Test Accuracy: {gb_accuracy}\")\n",
    "print(f\"Gradient Boosting Test F1 Score: {gb_f1}\")\n",
    "print(f\"Gradient Boosting Test Cohen's Kappa: {gb_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 0.74609375\n",
      "XGBoost Test F1 Score: 0.711702099428195\n",
      "XGBoost Test Cohen's Kappa: 0.1812456263121064\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model on the training dataset\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "if y_test_pred_xgb.ndim == 2:\n",
    "        y_test_pred_xgb = np.argmax(y_test_pred_xgb, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa for the XGBoost model\n",
    "xgb_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "xgb_kappa = cohen_kappa_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Test Accuracy: {xgb_accuracy}\")\n",
    "print(f\"XGBoost Test F1 Score: {xgb_f1}\")\n",
    "print(f\"XGBoost Test Cohen's Kappa: {xgb_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Power of the Feature Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy with Selected Features: 0.7183159722222222\n",
      "XGBoost Test F1 Score with Selected Features: 0.6695841191418528\n",
      "XGBoost Test Cohen's Kappa with Selected Features: 0.057371096586782855\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['rel_gamma_power', 'avg_clustering_coeff', 'degree_entropy', 'am_mean', 'abs_alpha_power']\n",
    "\n",
    "# Define features and labels for train and test sets using selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model_selected = xgb.XGBClassifier(objective='multi:softprob', num_class=2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model on the training dataset with selected features\n",
    "xgb_model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict the labels of the test set with selected features\n",
    "y_test_pred_xgb_selected = xgb_model_selected.predict(X_test_selected)\n",
    "\n",
    "if y_test_pred_xgb_selected.ndim == 2:\n",
    "    y_test_pred_xgb_selected = np.argmax(y_test_pred_xgb_selected, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Calculate accuracy, F1 score, and Cohen's kappa for the XGBoost model with selected features\n",
    "xgb_accuracy_selected = accuracy_score(y_test, y_test_pred_xgb_selected)\n",
    "xgb_f1_selected = f1_score(y_test, y_test_pred_xgb_selected, average='weighted')\n",
    "xgb_kappa_selected = cohen_kappa_score(y_test, y_test_pred_xgb_selected)\n",
    "\n",
    "print(f\"XGBoost Test Accuracy with Selected Features: {xgb_accuracy_selected}\")\n",
    "print(f\"XGBoost Test F1 Score with Selected Features: {xgb_f1_selected}\")\n",
    "print(f\"XGBoost Test Cohen's Kappa with Selected Features: {xgb_kappa_selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participand Dependent Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we trained the model using data from 34 participants and evaluated its predictive performance on the remaining 6 participants. Now, let's build a participant-dependent model, where we train a separate model for each participant using data from their first two trials and test it on the remaining trial for each activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n",
      "Mean Accuracy: 0.758203125\n",
      "Mean F1 Score: 0.7364961640066325\n",
      "Mean Cohen's Kappa: 0.3430289490966902\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each participant\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "kappas = []\n",
    "\n",
    "# Loop through each participant\n",
    "for subject in range(1, 41):\n",
    "    print(f\"{subject}\", end=' ')\n",
    "    # Extract data for the current subject\n",
    "    subject_data = data[data['file'].str.contains(f'sub_{subject}_')]\n",
    "    \n",
    "    # Split data into training and testing sets based on trial number\n",
    "    train_data = subject_data[subject_data['file'].str.contains('trial1|trial2')]\n",
    "    test_data = subject_data[subject_data['file'].str.contains('trial3')]\n",
    "    \n",
    "    # Define features and labels for training and testing sets\n",
    "    X_train = train_data.iloc[:, 3:-1]\n",
    "    y_train = train_data['Labels']\n",
    "    X_test = test_data.iloc[:, 3:-1]\n",
    "    y_test = test_data['Labels']\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy, F1 score, and Cohen's kappa\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    kappas.append(kappa)\n",
    "\n",
    "print() # Print a newline\n",
    "\n",
    "# Aggregate metrics\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_kappa = np.mean(kappas)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean Cohen's Kappa: {mean_kappa}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative power of the features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n",
      "Mean Accuracy (XGBoost with Selected Features): 0.729296875\n",
      "Mean F1 Score (XGBoost with Selected Features): 0.7146568083776572\n",
      "Mean Cohen's Kappa (XGBoost with Selected Features): 0.2396492589265286\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each participant\n",
    "accuracies_xgb_selected = []\n",
    "f1_scores_xgb_selected = []\n",
    "kappas_xgb_selected = []\n",
    "\n",
    "# Loop through each participant\n",
    "for subject in range(1, 41):\n",
    "    print(f\"{subject}\", end=' ')\n",
    "    # Extract data for the current subject\n",
    "    subject_data = data[data['file'].str.contains(f'sub_{subject}_')]\n",
    "    \n",
    "    # Split data into training and testing sets based on trial number\n",
    "    train_data = subject_data[subject_data['file'].str.contains('trial1|trial2')]\n",
    "    test_data = subject_data[subject_data['file'].str.contains('trial3')]\n",
    "    \n",
    "    # Define features and labels for training and testing sets using selected features\n",
    "    X_train_selected = train_data[selected_features]\n",
    "    y_train = train_data['Labels']\n",
    "    X_test_selected = test_data[selected_features]\n",
    "    y_test = test_data['Labels']\n",
    "    \n",
    "    # Define the XGBoost model\n",
    "    xgb_model_selected = xgb.XGBClassifier(objective='multi:softprob', num_class=2, random_state=42)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    xgb_model_selected.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred_xgb_selected = xgb_model_selected.predict(X_test_selected)\n",
    "\n",
    "    if y_test_pred_xgb_selected.ndim == 2:\n",
    "        y_test_pred_xgb_selected = np.argmax(y_test_pred_xgb_selected, axis=1)  # Convert probabilities to class labels\n",
    "    \n",
    "    # Calculate accuracy, F1 score, and Cohen's kappa\n",
    "    accuracy_xgb_selected = accuracy_score(y_test, y_test_pred_xgb_selected)\n",
    "    f1_xgb_selected = f1_score(y_test, y_test_pred_xgb_selected, average='weighted')\n",
    "    kappa_xgb_selected = cohen_kappa_score(y_test, y_test_pred_xgb_selected)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies_xgb_selected.append(accuracy_xgb_selected)\n",
    "    f1_scores_xgb_selected.append(f1_xgb_selected)\n",
    "    kappas_xgb_selected.append(kappa_xgb_selected)\n",
    "\n",
    "print()  # Print a newline\n",
    "# Aggregate metrics\n",
    "mean_accuracy_xgb_selected = np.mean(accuracies_xgb_selected)\n",
    "mean_f1_xgb_selected = np.mean(f1_scores_xgb_selected)\n",
    "mean_kappa_xgb_selected = np.mean(kappas_xgb_selected)\n",
    "\n",
    "print(f\"Mean Accuracy (XGBoost with Selected Features): {mean_accuracy_xgb_selected}\")\n",
    "print(f\"Mean F1 Score (XGBoost with Selected Features): {mean_f1_xgb_selected}\")\n",
    "print(f\"Mean Cohen's Kappa (XGBoost with Selected Features): {mean_kappa_xgb_selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n",
      "Mean Accuracy (XGBoost): 0.7447265625\n",
      "Mean F1 Score (XGBoost): 0.7187288440154825\n",
      "Mean Cohen's Kappa (XGBoost): 0.24045863450459723\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each participant\n",
    "accuracies_xgb = []\n",
    "f1_scores_xgb = []\n",
    "kappas_xgb = []\n",
    "\n",
    "# Loop through each participant\n",
    "for subject in range(1, 41):\n",
    "    print(f\"{subject}\", end=' ')\n",
    "    # Extract data for the current subject\n",
    "    subject_data = data[data['file'].str.contains(f'sub_{subject}_')]\n",
    "    \n",
    "    # Split data into training and testing sets based on trial number\n",
    "    train_data = subject_data[subject_data['file'].str.contains('trial1|trial2')]\n",
    "    test_data = subject_data[subject_data['file'].str.contains('trial3')]\n",
    "    \n",
    "    # Define features and labels for training and testing sets\n",
    "    X_train = train_data.iloc[:, 3:-1]\n",
    "    y_train = train_data['Labels']\n",
    "    X_test = test_data.iloc[:, 3:-1]\n",
    "    y_test = test_data['Labels']\n",
    "    \n",
    "    # Define the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=2, random_state=42)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    if y_test_pred_xgb.ndim == 2:\n",
    "        y_test_pred_xgb = np.argmax(y_test_pred_xgb, axis=1)  # Convert probabilities to class labels\n",
    "    \n",
    "    # Calculate accuracy, F1 score, and Cohen's kappa\n",
    "    accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "    f1_xgb = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "    kappa_xgb = cohen_kappa_score(y_test, y_test_pred_xgb)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    accuracies_xgb.append(accuracy_xgb)\n",
    "    f1_scores_xgb.append(f1_xgb)\n",
    "    kappas_xgb.append(kappa_xgb)\n",
    "\n",
    "print()  # Print a newline\n",
    "# Aggregate metrics\n",
    "mean_accuracy_xgb = np.mean(accuracies_xgb)\n",
    "mean_f1_xgb = np.mean(f1_scores_xgb)\n",
    "mean_kappa_xgb = np.mean(kappas_xgb)\n",
    "\n",
    "print(f\"Mean Accuracy (XGBoost): {mean_accuracy_xgb}\")\n",
    "print(f\"Mean F1 Score (XGBoost): {mean_f1_xgb}\")\n",
    "print(f\"Mean Cohen's Kappa (XGBoost): {mean_kappa_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam40",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
